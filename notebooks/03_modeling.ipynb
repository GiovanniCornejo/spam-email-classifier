{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "This notebook focuses on the training and evaluation of machine learning models for spam detection. We will use the preprocessed data from the previous step and apply various machine learning algorithms to classify messages as spam or ham. The notebook will cover model selection, training, and basic performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#model-training-and-evaluation)\n",
    "2. [Loading the Data](#loading-the-data)\n",
    "3. [Train-Test Split](#train-test-split)\n",
    "4. [Model Selection](#model-selection)\n",
    "5. [Exporting the Data](#exporting-the-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Preprocessed Data from Preprocessing Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (5572, 2)\n",
      "Original TF-IDF features shape: (5572, 5000)\n",
      "ROS data shape: (9650, 2)\n",
      "ROS TF-IDF features shape: (9650, 5000)\n",
      "RUS data shape: (1494, 2)\n",
      "RUS TF-IDF features shape: (1494, 3847)\n"
     ]
    }
   ],
   "source": [
    "# Define the directory paths\n",
    "processed_data_dir = 'data/processed'\n",
    "original_dir = os.path.join(processed_data_dir, 'original')\n",
    "ros_dir = os.path.join(processed_data_dir, 'ros')\n",
    "rus_dir = os.path.join(processed_data_dir, 'rus')\n",
    "\n",
    "# Load original data\n",
    "df_original = pd.read_csv(os.path.join(original_dir, 'original_data.csv'))\n",
    "X_tfidf_original = np.load(os.path.join(original_dir, 'original_tfidf_features.npy'))\n",
    "y_original = df_original['label']\n",
    "\n",
    "# Load ROS data\n",
    "df_ros = pd.read_csv(os.path.join(ros_dir, 'ros_data.csv'))\n",
    "X_tfidf_ros = np.load(os.path.join(ros_dir, 'ros_tfidf_features.npy'))\n",
    "y_ros = df_ros['label']\n",
    "\n",
    "# Load RUS data\n",
    "df_rus = pd.read_csv(os.path.join(rus_dir, 'rus_data.csv'))\n",
    "X_tfidf_rus = np.load(os.path.join(rus_dir, 'rus_tfidf_features.npy'))\n",
    "y_rus = df_rus['label']\n",
    "\n",
    "datasets = {\n",
    "    'original': (X_tfidf_original, y_original),\n",
    "    'ros': (X_tfidf_ros, y_ros),\n",
    "    'rus': (X_tfidf_rus, y_rus)\n",
    "}\n",
    "\n",
    "# Print the shapes of the loaded data\n",
    "print(\"Original data shape:\", df_original.shape)\n",
    "print(\"Original TF-IDF features shape:\", X_tfidf_original.shape)\n",
    "print(\"ROS data shape:\", df_ros.shape)\n",
    "print(\"ROS TF-IDF features shape:\", X_tfidf_ros.shape)\n",
    "print(\"RUS data shape:\", df_rus.shape)\n",
    "print(\"RUS TF-IDF features shape:\", X_tfidf_rus.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random state is used to control the randomness involved in algorithms that rely on random processes,\n",
    "# such as data shuffling, train-test splits, or random initialization of model parameters.\n",
    "rs = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Split the preprocessed data into training and testing sets to evaluate our model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=rs):\n",
    "    \"\"\"\n",
    "    Split data into training and testing sets.\n",
    "    \n",
    "    ## Parameters\n",
    "    `X`: ndarray \n",
    "        Features.\n",
    "    `y`: ndarray\n",
    "        Labels.\n",
    "    `test_size`: float, default=0.2\n",
    "        Proportion of the dataset to include in the test split.\n",
    "    `random_state`: int, default=42\n",
    "        Random seed for reproducibility.\n",
    "    \n",
    "    ## Returns\n",
    "    `dict`: Dictionary containing training and testing sets (X_train, X_test, y_train, y_test).\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    return {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}\n",
    "\n",
    "# Split the data for all datasets\n",
    "data_splits = {\n",
    "    'original': split_data(X_tfidf_original, y_original),\n",
    "    'ros': split_data(X_tfidf_ros, y=y_ros),\n",
    "    'rus': split_data(X_tfidf_rus, y_rus)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "*Note*: The `Train and Evaluate` cell blocks may take a while to finish computing. The notebooks are designed in a way such that not every model has to be ran (i.e., Gradient Boosting takes quite a while to complete).\n",
    "\n",
    "In this section, we will explore and evaluate different machine learning models to classify spam emails. We will use various models to understand which one performs best for our dataset. The models we will cover include:\n",
    "\n",
    "1. [Logistic Regression](#1-logistic-regression)\n",
    "2. [Decision Tree](#2-decision-tree)\n",
    "3. [Random Forest](#3-random-forest)\n",
    "4. [Gradient Boosting](#4-gradient-boosting)\n",
    "5. [Support Vector Machine (SVM)](#5-support-vector-machine-svm)\n",
    "6. [k-Nearest Neighbors (k-NN)](#6-k-nearest-neighbors-k-nn)\n",
    "7. [Naive Bayes](#7-naive-bayes)\n",
    "\n",
    "*Note*: Certain algorithms like Random Forests can handle imbalance better.\n",
    "\n",
    "We will analyze the following metrics in our overall evaluation of each model:\n",
    "* **Accuracy** - The proportion of correctly classified instances among the total number of instances. It provides an overall assessment of the model's correctness.\n",
    "\n",
    "* **Precision** - Also known as positive predictive value, measures the proportion of correctly predicted positive instances (true positives) among all predicted positive instances (true positives + false positives). It reflects the model's ability to avoid false positives.\n",
    "    - \"Of all the instances predicted as positive, how many are actually positive?\"\n",
    "\n",
    "* **Recall** - Also known as sensitivity or true positive rate, measures the proportion of correctly predicted positive instances (true positives) among all actual positive instances (true positives + false negatives). It reflects the model's ability to identify all relevant instances.\n",
    "    - \"Of all the actual positive instances, how many were correctly predicted as positive?\"\n",
    "\n",
    "* **F1** - The harmonic mean of precision and recall. It provides a balance between precision and recall and is especially useful when dealing with imbalanced datasets.\n",
    "\n",
    "*Note*: Accuracy might not be the best metric. Precision, recall, and F1-score can better evaluate the model's performance on imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dictionary to store each model and their corresponding metrics\n",
    "evaluation_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a machine learning model.\n",
    "    \n",
    "    ## Parameters\n",
    "    `model`: sklearn estimator object\n",
    "        The model to evaluate.\n",
    "    `X_train`: array-like\n",
    "        Training features.\n",
    "    `X_test`: array-like\n",
    "        Testing features.\n",
    "    `y_train`: array-like\n",
    "        Training labels.\n",
    "    `y_test`: array-like\n",
    "        Testing labels.\n",
    "    \n",
    "    ## Returns\n",
    "    `dict`: A dictionary of evaluation metrics (accuracy, precision, recall, f1) along with the fitted model and predictions.\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Store evaluation metrics, trained model, and predictions in a dictionary\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(model, X, y, n_splits=10, random_state=rs):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to evaluate the performance of a machine learning model.\n",
    "    \n",
    "    ## Parameters\n",
    "    `model`: sklearn estimator object\n",
    "        The model to evaluate.\n",
    "    `X`: array-like\n",
    "        Features.\n",
    "    `y`: array-like\n",
    "        Labels.\n",
    "    `n_splits`: int, default=10\n",
    "        Number of folds for cross-validation.\n",
    "    `random_state`: int, default=42\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    ## Returns\n",
    "    `dict`: Dictionary containing evaluation metrics (accuracy, precision, recall, f1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize StratifiedKFold cross-validator\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Initialize dictionary to store evaluation metrics\n",
    "    scores = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "    # Iterate over each fold\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Split the data into training and testing sets for the current fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate evaluation metrics for the current fold\n",
    "        scores['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        scores['precision'].append(precision_score(y_test, y_pred))\n",
    "        scores['recall'].append(recall_score(y_test, y_pred))\n",
    "        scores['f1'].append(f1_score(y_test, y_pred))\n",
    "\n",
    "    # Calculate average metrics across all folds\n",
    "    metrics = {}\n",
    "    for metric, values in scores.items():\n",
    "        metrics[metric] = np.mean(values)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, data_splits, datasets = None):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model using train-test split and cross-validation methods.\n",
    "\n",
    "    ## Parameters\n",
    "    `model`: sklearn estimator\n",
    "        The model to be trained and evaluated.\n",
    "    `data_splits`: dict\n",
    "        Dictionary containing the train-test splits for each dataset.\n",
    "    `datasets`: dict (optional)\n",
    "        Dictionary containing the full datasets for cross-evaluation.\n",
    "    \n",
    "    ## Returns\n",
    "    `dict`: A dictionary with evaluation metrics for each dataset and method, and the fitted model (from train-test split) along with its predictions.\n",
    "\n",
    "    \"\"\"\n",
    "    # Dictionary to store evaluation metrics for each dataset and method\n",
    "    evaluation_results = {}\n",
    "\n",
    "    for dataset_name, dataset_splits in data_splits.items():\n",
    "        # Extract the training and testing splits\n",
    "        X_train, y_train = dataset_splits['X_train'], dataset_splits['y_train']\n",
    "        X_test, y_test = dataset_splits['X_test'], dataset_splits['y_test']\n",
    "\n",
    "        # Evaluate the model using train-test split method\n",
    "        evaluation_results[dataset_name] = {}\n",
    "        evaluation_results[dataset_name]['train_test_split'] = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Evaluate the model using cross-validation method\n",
    "    if datasets:\n",
    "        for dataset_name, (X, y) in datasets.items():\n",
    "            evaluation_results[dataset_name]['cross_validation'] = cross_validate_model(model, X, y)\n",
    "\n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_summary(model_name, evaluation_results):\n",
    "    \"\"\"\n",
    "    Print the evaluation summary for each dataset and method.\n",
    "\n",
    "    ## Parameters\n",
    "    `evaluation_results`: dict\n",
    "        Dictionary with evaluation metrics for each dataset and method.\n",
    "    \"\"\"\n",
    "    # Title and Column Headers\n",
    "    print(f\"{model_name} Results\\n\", '-'*(len(model_name)+8), sep='')\n",
    "    if evaluation_results['ros'].get('cross_validation', None):\n",
    "        print(\"{:^10} | {:^10} | {:^18} | {:^18} |\".format(\"Dataset\", \"Metric\", \"Train Test Split\", \"Cross Validation\"))\n",
    "    else:\n",
    "        print(\"{:^10} | {:^10} | {:^18} |\".format(\"Dataset\", \"Metric\", \"Train Test Split\"))\n",
    "\n",
    "    for dataset_name, methods in evaluation_results.items():\n",
    "        # Curren Dataset (Original, ROS, RUS)\n",
    "        print(f\"{dataset_name:^10} | \", end=\"\")\n",
    "        metrics_split = methods['train_test_split']\n",
    "        metrics_cross = methods.get('cross_validation', None)\n",
    "        first_metric = True\n",
    "        for metric_name in metrics_split.keys():\n",
    "            # SKip the model and predictions\n",
    "            if metric_name == 'model' or metric_name == 'y_pred':\n",
    "                continue\n",
    "\n",
    "            v1 = \"{:.4f} %\".format(metrics_split[metric_name] * 100) # Train-Test Split\n",
    "            v2 = \"{:.4f} %\".format(metrics_cross.get(metric_name) * 100) if metrics_cross else None # Cross Validation\n",
    "            if not first_metric:\n",
    "                print(\"{:^10} | \".format(''), end=\"\")\n",
    "            first_metric = False\n",
    "            print(\"{:<10} | \".format(metric_name.capitalize()), end=\"\")\n",
    "            if v2:\n",
    "                print(\"{:^18} | {:^18} |\".format(v1, v2))\n",
    "            else:\n",
    "                print(\"{:^18} |\".format(v1))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression\n",
    "\n",
    "Logistic Regression is a linear model commonly used for binary classification problems. It predicts the probability of a binary outcome using a logistic function. Also known as the sigmoid function, this outputs a probability value that is then mapped to two possible classes. \n",
    "\n",
    "It often serves as a good baseline model despite its simplicity, making it a valuable tool for initial analysis before moving on to more complex models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the Logistic Regression model\n",
    "logreg_model = LogisticRegression(random_state=rs)\n",
    "\n",
    "# Train and evaluate the model\n",
    "logreg_evaluation_results = train_and_evaluate_model(logreg_model, data_splits, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "---------------------------\n",
      " Dataset   |   Metric   |  Train Test Split  |  Cross Validation  |\n",
      " original  | Accuracy   |     95.8744 %      |     95.8003 %      |\n",
      "           | Precision  |     99.0476 %      |     97.7681 %      |\n",
      "           | Recall     |     69.7987 %      |     70.2937 %      |\n",
      "           | F1         |     81.8898 %      |     81.7395 %      |\n",
      "\n",
      "   ros     | Accuracy   |     99.0155 %      |     98.8497 %      |\n",
      "           | Precision  |     98.9648 %      |     99.0034 %      |\n",
      "           | Recall     |     99.0674 %      |     98.6944 %      |\n",
      "           | F1         |     99.0161 %      |     98.8475 %      |\n",
      "\n",
      "   rus     | Accuracy   |     92.9766 %      |     93.4425 %      |\n",
      "           | Precision  |     98.4848 %      |     98.2318 %      |\n",
      "           | Recall     |     87.2483 %      |     88.4937 %      |\n",
      "           | F1         |     92.5267 %      |     93.0188 %      |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation summary\n",
    "print_evaluation_summary('Logistic Regression', logreg_evaluation_results)\n",
    "evaluation_results['Logistic Regression'] = logreg_evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results presented:\n",
    "\n",
    "* Both the train-test split and cross-validation methods yielded similar results across all metrics and datasets, suggesting consistency in model performance.\n",
    "\n",
    "1. **Original Dataset**:\n",
    "    * Despite not addressing class imbalance, the original dataset achieved near-perfect accuracy and precision scores. However, it struggled with recall and F1 scores.\n",
    "    * The above indicates potential issues with correctly identifying positive instances.\n",
    "\n",
    "2. **ROS (Random Over-Sampling) Dataset**:\n",
    "    * The ROS dataset consistently outperformed both the original and RUS datasets across all metrics.\n",
    "\n",
    "3. **RUS (Random Under-Sampling) Dataset**:\n",
    "    * The RUS dataset also exhibited improved performance compared to the original, albeit slightly lower than the ROS dataset.\n",
    "\n",
    "Overall Summary:\n",
    "* Both oversampling (ROS) and undersampling (RUS) techniques effectively addressed the class imbalance issue, resulting in improved model performance across all metrics.\n",
    "* The ROS dataset showed the most promising results, achieving high metrics across both train-test split and cross-validation methods.\n",
    "* These findings suggest that addressing class imbalance through sampling techniques significantly enhances the model's ability to generalize and perform well on both training and unseen data.\n",
    "\n",
    "**Note: We will only perform the train-test split method for subsequent models to decrease computation time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree\n",
    "\n",
    "Decision Tree is a non-linear supervised learning algorithm used for both classification and regression tasks. It works by splitting the data into subsets based on the value of input features, forming a tree-like model of decisions. \n",
    "* Each internal node of the tree represents a decision based on an attribute\n",
    "* Each branch represents the outcome of the decision, and\n",
    "* Each leaf node represents a class label or a continuous value.\n",
    "\n",
    "Unlike linear models, Decision Trees can capture non-linear relationships between features and the target variable. This flexibility allows the model to fit more complex patterns in the data, which can be beneficial for identifying spam emails.\n",
    "\n",
    "Decision Trees perform implicit feature selection by choosing the most informative features for splitting the data. This property helps in reducing the dimensionality and removing irrelevant features from our spam email dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=rs)\n",
    "\n",
    "# Train and evaluate the Decision Tree model\n",
    "dt_evaluation_results= train_and_evaluate_model(dt_model, data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Results\n",
      "---------------------\n",
      " Dataset   |   Metric   |  Train Test Split  |\n",
      " original  | Accuracy   |     95.8744 %      |\n",
      "           | Precision  |     85.0340 %      |\n",
      "           | Recall     |     83.8926 %      |\n",
      "           | F1         |     84.4595 %      |\n",
      "\n",
      "   ros     | Accuracy   |     97.8238 %      |\n",
      "           | Precision  |     95.8292 %      |\n",
      "           | Recall     |     100.0000 %     |\n",
      "           | F1         |     97.8702 %      |\n",
      "\n",
      "   rus     | Accuracy   |     90.9699 %      |\n",
      "           | Precision  |     89.6104 %      |\n",
      "           | Recall     |     92.6174 %      |\n",
      "           | F1         |     91.0891 %      |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation summary\n",
    "print_evaluation_summary('Decision Tree', dt_evaluation_results)\n",
    "evaluation_results['Decision Tree'] = dt_evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results presented:\n",
    "\n",
    "The ROS dataset once again demonstrated strong performance, with the Decision Tree model achieving high accuracy, precision, recall, and F1 score. While there was a slight decrease in performance compared to the Logistic Regression model, the differences were minimal, indicating that the ROS dataset remains a viable option for building a spam email classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random Forest\n",
    "\n",
    "Random Forest is a versatile ensemble learning method for classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputs the mode of the classes (classification) or the mean prediction (regression) of the individual trees.\n",
    "\n",
    "By aggregating predictions from multiple decision trees, Random Forest reduces the risk of overfitting and performs well on imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train and evaluate the model\n",
    "rf_evaluation_results = train_and_evaluate_model(rf_model, data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results\n",
      "---------------------\n",
      " Dataset   |   Metric   |  Train Test Split  |\n",
      " original  | Accuracy   |     97.5785 %      |\n",
      "           | Precision  |     100.0000 %     |\n",
      "           | Recall     |     81.8792 %      |\n",
      "           | F1         |     90.0369 %      |\n",
      "\n",
      "   ros     | Accuracy   |     99.9482 %      |\n",
      "           | Precision  |     99.8965 %      |\n",
      "           | Recall     |     100.0000 %     |\n",
      "           | F1         |     99.9482 %      |\n",
      "\n",
      "   rus     | Accuracy   |     93.3110 %      |\n",
      "           | Precision  |     96.4029 %      |\n",
      "           | Recall     |     89.9329 %      |\n",
      "           | F1         |     93.0556 %      |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Output summary of evaluation metrics\n",
    "print_evaluation_summary('Random Forest', rf_evaluation_results)\n",
    "evaluation_results['Random Forest'] = rf_evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results presented:\n",
    "\n",
    "Random Forest excelled on the ROS dataset, achieving near-perfect accuracy, precision, recall, and F1 score, indicating excellent performance on oversampled data. This should be no surprised as mentioned earlier that this model can handle imbalance better.\n",
    "\n",
    "**Note: Due to the consistency of ROS performance, we will be disregarding the RUS datasets to reduce computation time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gradient Boosting\n",
    "\n",
    "Gradient Boosting is an ensemble learning technique used for regression and classification tasks. It builds a model in a stage-wise fashion, and it generalizes by allowing optimization of an arbitrary differentiable loss function. Each new model attempts to correct the errors made by the previously trained model. This iterative process results in a strong predictive model.\n",
    "\n",
    "For our spam email classification task, Gradient Boosting can potentially capture subtle patterns in the data that simpler models might miss, leading to improved accuracy and robustness in detecting spam emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Remove rus from data_splits\n",
    "data_splits = {\n",
    "    'original': data_splits['original'],\n",
    "    'ros': data_splits['ros']\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(random_state=rs)\n",
    "\n",
    "# Train and evaluate the model on the datasets\n",
    "gb_evaluation_results = train_and_evaluate_model(gb_model, data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Results\n",
      "-------------------------\n",
      " Dataset   |   Metric   |  Train Test Split  |\n",
      " original  | Accuracy   |     96.1435 %      |\n",
      "           | Precision  |     94.1667 %      |\n",
      "           | Recall     |     75.8389 %      |\n",
      "           | F1         |     84.0149 %      |\n",
      "\n",
      "   ros     | Accuracy   |     93.0052 %      |\n",
      "           | Precision  |     96.0089 %      |\n",
      "           | Recall     |     89.7409 %      |\n",
      "           | F1         |     92.7691 %      |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation summary\n",
    "print_evaluation_summary('Gradient Boosting', gb_evaluation_results)\n",
    "evaluation_results['Gradient Boosting'] = gb_evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results presented:\n",
    "\n",
    "The Gradient Boosting model performed well on the ROS dataset, though it did not achieve the same high levels of performance as the Random Forest model. The precision and recall values suggest a strong ability to correctly identify positive instances, but with slightly more false positives or negatives compared to the Random Forest results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Support Vector Machine (SVM)\n",
    "\n",
    "Support Vector Machine (SVM) is a powerful supervised learning algorithm that can be used for both classification and regression tasks. It works by finding the hyperplane that best separates the data into different classes. The SVM is particularly effective in high-dimensional spaces and when the number of dimensions is greater than the number of samples.\n",
    "\n",
    "SVM is suitable for datasets with a large number of features, which is the case with our TF-IDF vectorized text data. Although our dataset has been balanced using ROS, SVM can handle imbalanced datasets well by maximizing the margin between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=rs)\n",
    "\n",
    "# Train and evaluate the model on the datasets\n",
    "svm_evaluation_results = train_and_evaluate_model(svm_model, data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Results\n",
      "------------------------------\n",
      " Dataset   |   Metric   |  Train Test Split  |\n",
      " original  | Accuracy   |     98.2960 %      |\n",
      "           | Precision  |     100.0000 %     |\n",
      "           | Recall     |     87.2483 %      |\n",
      "           | F1         |     93.1900 %      |\n",
      "\n",
      "   ros     | Accuracy   |     99.7927 %      |\n",
      "           | Precision  |     99.7927 %      |\n",
      "           | Recall     |     99.7927 %      |\n",
      "           | F1         |     99.7927 %      |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation summary\n",
    "print_evaluation_summary('Support Vector Machine', svm_evaluation_results)\n",
    "evaluation_results['Support Vector Machine'] = svm_evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results presented:\n",
    "\n",
    "The identical scores across all four metrics suggest that the SVM model performed exceptionally well, maintaining a perfect balance between precision and recall. This indicates that the model has a very high capability of distinguishing between spam and non-spam emails without bias towards either class. The performance is on par with the best-performing models in our evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. k-Nearest Neighbors (k-NN)\n",
    "\n",
    "k-Nearest Neighbors (k-NN) is a simple, instance-based learning algorithm that classifies a data point based on the majority class among its k nearest neighbors in the feature space. It is a non-parametric method, meaning it makes no explicit assumptions about the form of the function that relates the features to the target variable. Since k-NN makes predictions based on local neighborhoods, it can effectively handle balanced datasets, such as those created by the ROS method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the k-NN model with k=5 (common default value)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train and evaluate the model on the datasets\n",
    "knn_evaluation_results = train_and_evaluate_model(knn_model, data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-Nearest Neighbors Results\n",
      "---------------------------\n",
      " Dataset   |   Metric   |  Train Test Split  |\n",
      " original  | Accuracy   |     91.5695 %      |\n",
      "           | Precision  |     100.0000 %     |\n",
      "           | Recall     |     36.9128 %      |\n",
      "           | F1         |     53.9216 %      |\n",
      "\n",
      "   ros     | Accuracy   |     98.6010 %      |\n",
      "           | Precision  |     99.8936 %      |\n",
      "           | Recall     |     97.3057 %      |\n",
      "           | F1         |     98.5827 %      |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation summary\n",
    "print_evaluation_summary(\"k-Nearest Neighbors\", knn_evaluation_results)\n",
    "evaluation_results['k-Nearest Neighbors'] = knn_evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results presented:\n",
    "\n",
    "The k-NN model performs very well on the ROS dataset, with high precision and recall scores. The high accuracy and F1 score further demonstrate the model's effectiveness in classifying the dataset. However, it is slightly worse performance than the best-performing models in our evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Naive Bayes\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on Bayes' theorem with the \"naive\" assumption of independence between features. It is commonly used for text classification tasks, making it suitable for our dataset of spam email detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize the Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Train and evaluate the model on the ROS dataset\n",
    "nb_evaluation_results = train_and_evaluate_model(nb_model, data_splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results\n",
      "-------------------\n",
      " Dataset   |   Metric   |  Train Test Split  |\n",
      " original  | Accuracy   |     96.6816 %      |\n",
      "           | Precision  |     99.1228 %      |\n",
      "           | Recall     |     75.8389 %      |\n",
      "           | F1         |     85.9316 %      |\n",
      "\n",
      "   ros     | Accuracy   |     98.1865 %      |\n",
      "           | Precision  |     98.2365 %      |\n",
      "           | Recall     |     98.1347 %      |\n",
      "           | F1         |     98.1856 %      |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation summary\n",
    "print_evaluation_summary(\"Naive Bayes\", nb_evaluation_results)\n",
    "evaluation_results['Naive Bayes'] = nb_evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results presented:\n",
    "\n",
    "Naive Bayes demonstrates its effectiveness in text classification tasks like spam email detection, offering a computationally efficient solution with competitive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the Data\n",
    "\n",
    "After training and evaluating each model, we export it to the `results` folder for use in the evaluation step next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Directory to store the results and split data\n",
    "results_dir = 'results/'\n",
    "data_dir = 'data/processed/split'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(name=results_dir, exist_ok=True)\n",
    "os.makedirs(name=data_dir, exist_ok=True)\n",
    "\n",
    "# Save evaluation_results to a pickle file\n",
    "with open(os.path.join(results_dir, 'evaluation_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(evaluation_results, f)\n",
    "\n",
    "# Save the data_splits dictionary to a pickle file\n",
    "with open(os.path.join(data_dir, 'data_splits.pkl'), 'wb') as f:\n",
    "    pickle.dump(data_splits, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Insights\n",
    "\n",
    "### Impact of Data Preprocessing Techniques\n",
    "* Oversampling (ROS) and undersampling (RUS) techniques significantly improved model performance for the original imbalanced dataset.\n",
    "\n",
    "### Performance of Different Models\n",
    "* **Logistic Regression**: Achieved high accuracy and precision but had lower recall and F1-score on the original dataset. ROS and RUS techniques improved overall performance.\n",
    "\n",
    "* **Decision Tree**: Showed promising results, especially on the ROS dataset, with high accuracy and precision.\n",
    "\n",
    "* **Random Forest**: Outperformed other models on the ROS dataset, demonstrating high accuracy, precision, and recall.\n",
    "\n",
    "* **Gradient Boosting**: Achieved competitive performance, particularly on the ROS dataset, with high accuracy and precision; however showed low recall scores.\n",
    "\n",
    "* **Support Vector Machine (SVM)**: Demonstrated excellent performance on the ROS dataset, with high accuracy, precision, recall, and F1-score.\n",
    "\n",
    "* **k-Nearest Neighbors (k-NN)**: Showed good performance, especially on the ROS dataset, with high accuracy, precision, and recall.\n",
    "\n",
    "* **Naive Bayes**: Achieved high accuracy and precision on the ROS dataset, with competitive performance compared to other models.\n",
    "\n",
    "### Key Insights\n",
    "* The ROS technique consistently improved model performance across different algorithms, indicating its effectiveness in handling class imbalance.\n",
    "* Certain models such as Random Forest and SVM performed exceptionally well on the ROS dataset, suggesting their suitability for this classification task.\n",
    "\n",
    "### Next Steps\n",
    "* **Model Evaluation**: Analyze the performance of each model using various evaluation metrics, including confusion matrices, classification reports, and ROC curves.\n",
    "\n",
    "* **Model Comparison**: Further compare the performance of different models side by side to identify the most effective approach for our email classification task.\n",
    "\n",
    "* **Visualization**: Utilize visualizations to present the evaluation results in an intuitive and understandable manner.\n",
    "\n",
    "* **Further Analysis**: Explore additional avenues for analysis, including feature importance plots. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
