{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "This notebook focuses on the training and evaluation of machine learning models for spam detection. We will use the preprocessed data from the previous step and apply various machine learning algorithms to classify messages as spam or ham. The notebook will cover model selection, training, hyperparameter tuning, and performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#model-training-and-evaluation)\n",
    "2. [Loading the Data](#loading-the-data)\n",
    "3. [Train-Test Split](#train-test-split)\n",
    "4. [Model Selection](#model-selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Preprocessed Data from Preprocessing Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (5572, 2)\n",
      "Original TF-IDF features shape: (5572, 5000)\n",
      "ROS data shape: (9650, 2)\n",
      "ROS TF-IDF features shape: (9650, 5000)\n",
      "RUS data shape: (1494, 2)\n",
      "RUS TF-IDF features shape: (1494, 3847)\n"
     ]
    }
   ],
   "source": [
    "# Define the directory paths\n",
    "processed_data_dir = 'data/processed'\n",
    "original_dir = os.path.join(processed_data_dir, 'original')\n",
    "ros_dir = os.path.join(processed_data_dir, 'ros')\n",
    "rus_dir = os.path.join(processed_data_dir, 'rus')\n",
    "\n",
    "# Load original data\n",
    "df_original = pd.read_csv(os.path.join(original_dir, 'original_data.csv'))\n",
    "X_tfidf_original = np.load(os.path.join(original_dir, 'original_tfidf_features.npy'))\n",
    "y_original = df_original['label']\n",
    "\n",
    "# Load ROS data\n",
    "df_ros = pd.read_csv(os.path.join(ros_dir, 'ros_data.csv'))\n",
    "X_tfidf_ros = np.load(os.path.join(ros_dir, 'ros_tfidf_features.npy'))\n",
    "y_ros = df_ros['label']\n",
    "\n",
    "# Load RUS data\n",
    "df_rus = pd.read_csv(os.path.join(rus_dir, 'rus_data.csv'))\n",
    "X_tfidf_rus = np.load(os.path.join(rus_dir, 'rus_tfidf_features.npy'))\n",
    "y_rus = df_rus['label']\n",
    "\n",
    "datasets = {\n",
    "    'original': (X_tfidf_original, y_original),\n",
    "    'ros': (X_tfidf_ros, y_ros),\n",
    "    'rus': (X_tfidf_rus, y_rus)\n",
    "}\n",
    "\n",
    "# Print the shapes of the loaded data\n",
    "print(\"Original data shape:\", df_original.shape)\n",
    "print(\"Original TF-IDF features shape:\", X_tfidf_original.shape)\n",
    "print(\"ROS data shape:\", df_ros.shape)\n",
    "print(\"ROS TF-IDF features shape:\", X_tfidf_ros.shape)\n",
    "print(\"RUS data shape:\", df_rus.shape)\n",
    "print(\"RUS TF-IDF features shape:\", X_tfidf_rus.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Split the preprocessed data into training and testing sets to evaluate our model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 42\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=rs):\n",
    "    \"\"\"\n",
    "    Split data into training and testing sets.\n",
    "    \n",
    "    ## Parameters\n",
    "    `X`: ndarray \n",
    "        Features.\n",
    "    `y`: ndarray\n",
    "        Labels.\n",
    "    `test_size`: float, default=0.2\n",
    "        Proportion of the dataset to include in the test split.\n",
    "    `random_state`: int, default=42\n",
    "        Random seed for reproducibility.\n",
    "    \n",
    "    ## Returns\n",
    "    `dict`: Dictionary containing training and testing sets (X_train, X_test, y_train, y_test).\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    return {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}\n",
    "\n",
    "# Split the data for all datasets\n",
    "data_splits = {\n",
    "    'original': split_data(X_tfidf_original, y_original),\n",
    "    'ros': split_data(X_tfidf_ros, y=y_ros),\n",
    "    'rus': split_data(X_tfidf_rus, y_rus)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "*Note*: The `Train and Evaluate` cell blocks may take a while to finish computing\n",
    "\n",
    "In this section, we will explore and evaluate different machine learning models to classify spam emails. We will use various models to understand which one performs best for our dataset. The models we will cover include:\n",
    "\n",
    "1. [Logistic Regression](#1-logistic-regression)\n",
    "2. [Decision Tree](#2-decision-tree)\n",
    "3. [Random Forest](#3-random-forest)\n",
    "4. [Gradient Boosting](#4-gradient-boosting)\n",
    "5. Support Vector Machine (SVM)\n",
    "6. k-Nearest Neighbors (k-NN)\n",
    "7. Naive Bayes\n",
    "\n",
    "*Note*: Certain algorithms like Random Forests and Gradient Boosting can handle imbalance better.\n",
    "\n",
    "We will analyze the following metrics in our evaluation of each model:\n",
    "* **Accuracy** - The proportion of correctly classified instances among the total number of instances. It provides an overall asssessment of the model's correctness.\n",
    "\n",
    "* **Precision** - Also known as positive predictive value, measures the proportion of correctly predicted positive instances (true positives) among all predicted positive instances (true positives + false positives). It reflects the model's ability to avoid false positives.\n",
    "    - \"Of all the instances predicted as positive, how many are actually positive?\"\n",
    "\n",
    "* **Recall** - Also known as sensitivity or true positive rate, measures the proportion of correctly predicted positive instances (true positives) among all actual positive instances (true positives + false negatives). It reflects the model's ability to identify all relevant instances.\n",
    "    - \"Of all the actual positive instances, how many were correctly predicted as positive?\"\n",
    "\n",
    "* **F1** - The harmonic mean of precision and recall. It provides a balance between precision and recall and is especially useful when dealing with imbalanced datasets.\n",
    "\n",
    "*Note*: Accuracy might not be the best metric. Precision, recall, and F1-score can better evaluate the model's performance on imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a machine learning model.\n",
    "    \n",
    "    ## Parameters\n",
    "    `model`: sklearn estimator object\n",
    "        The model to evaluate.\n",
    "    `X_train`: array-like\n",
    "        Training features.\n",
    "    `X_test`: array-like\n",
    "        Testing features.\n",
    "    `y_train`: array-like\n",
    "        Training labels.\n",
    "    `y_test`: array-like\n",
    "        Testing labels.\n",
    "    \n",
    "    ## Returns\n",
    "    `tuple[dict, model]`: Tuple containing dictionary of evaluation metrics (accuracy, precision, recall, f1) and the fitted model.\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Store evaluation metrics in a dictionary\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    return metrics, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(model, X, y, n_splits=10, random_state=rs):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to evaluate the performance of a machine learning model.\n",
    "    \n",
    "    ## Parameters\n",
    "    `model`: sklearn estimator object\n",
    "        The model to evaluate.\n",
    "    `X`: array-like\n",
    "        Features.\n",
    "    `y`: array-like\n",
    "        Labels.\n",
    "    `n_splits`: int, default=10\n",
    "        Number of folds for cross-validation.\n",
    "    `random_state`: int, default=42\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    ## Returns\n",
    "    `dict`: Dictionary containing evaluation metrics (accuracy, precision, recall, f1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize StratifiedKFold cross-validator\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Initialize dictionary to store evaluation metrics\n",
    "    scores = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "    # Iterate over each fold\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Split the data into training and testing sets for the current fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate evaluation metrics for the current fold\n",
    "        scores['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        scores['precision'].append(precision_score(y_test, y_pred))\n",
    "        scores['recall'].append(recall_score(y_test, y_pred))\n",
    "        scores['f1'].append(f1_score(y_test, y_pred))\n",
    "\n",
    "    # Calculate average metrics across all folds\n",
    "    metrics = {}\n",
    "    for metric, values in scores.items():\n",
    "        metrics[metric] = np.mean(values)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, data_splits, datasets = None):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model using train-test split and cross-validation methods.\n",
    "\n",
    "    ## Parameters\n",
    "    `model`: sklearn estimator\n",
    "        The model to be trained and evaluated.\n",
    "    `data_splits`: dict\n",
    "        Dictionary containing the train-test splits for each dataset.\n",
    "    `datasets`: dict (optional)\n",
    "        Dictionary containing the full datasets for cross-evaluation.\n",
    "    \n",
    "    ## Returns\n",
    "    `tuple[dict, model]`: Tuple containing a dictionary with evaluation metrics for each dataset and method, and the fitted model (from train-test split).\n",
    "\n",
    "    \"\"\"\n",
    "    # Dictionary to store evaluation metrics for each dataset and method\n",
    "    evaluation_results = {}\n",
    "    output_model = model\n",
    "\n",
    "    for dataset_name, dataset_splits in data_splits.items():\n",
    "        # Extract the training and testing splits\n",
    "        X_train, y_train = dataset_splits['X_train'], dataset_splits['y_train']\n",
    "        X_test, y_test = dataset_splits['X_test'], dataset_splits['y_test']\n",
    "\n",
    "        # Evaluate the model using train-test split method\n",
    "        evaluation_results[dataset_name] = {}\n",
    "        evaluation_results[dataset_name]['train_test_split'], model = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Return trained ROS model (best model based on below evaluations)\n",
    "        if dataset_name == 'ros':\n",
    "            output_model = model\n",
    "\n",
    "    # Evaluate the model using cross-validation method\n",
    "    if datasets:\n",
    "        for dataset_name, (X, y) in datasets.items():\n",
    "            evaluation_results[dataset_name]['cross_validation'] = cross_validate_model(model, X, y)\n",
    "\n",
    "    return evaluation_results, output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_summary(model_name, evaluation_results):\n",
    "    \"\"\"\n",
    "    Print the evaluation summary for each dataset and method.\n",
    "\n",
    "    ## Parameters\n",
    "    `evaluation_results`: dict\n",
    "        Dictionary with evaluation metrics for each dataset and method.\n",
    "    \"\"\"\n",
    "    print(f\"{model_name} Results\\n\", '-'*(len(model_name)+8), sep='')\n",
    "    if evaluation_results['ros'].get('cross_validation', None):\n",
    "        print(\"{:^10} | {:^10} | {:^18} | {:^18} |\".format(\"Dataset\", \"Metric\", \"Train Test Split\", \"Cross Validation\"))\n",
    "    else:\n",
    "        print(\"{:^10} | {:^10} | {:^18} |\".format(\"Dataset\", \"Metric\", \"Train Test Split\"))\n",
    "\n",
    "    for dataset_name, methods in evaluation_results.items():\n",
    "        print(f\"{dataset_name:^10} | \", end=\"\")\n",
    "        metrics_split = methods['train_test_split']\n",
    "        metrics_cross = methods.get('cross_validation', None)\n",
    "        first_metric = True\n",
    "        for metric_name in metrics_split.keys():\n",
    "            v1 = \"{:.3f} %\".format(metrics_split[metric_name] * 100)\n",
    "            v2 = \"{:.3f} %\".format(metrics_cross.get(metric_name) * 100) if metrics_cross else None\n",
    "            if not first_metric:\n",
    "                print(\"{:^10} | \".format(''), end=\"\")\n",
    "            first_metric = False\n",
    "            print(\"{:<10} | \".format(metric_name.capitalize()), end=\"\")\n",
    "            if v2:\n",
    "                print(\"{:^18} | {:^18} |\".format(v1, v2))\n",
    "            else:\n",
    "                print(\"{:^18} |\".format(v1))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression\n",
    "\n",
    "Logistic Regression is a linear model commonly used for binary classification problems. It predicts the probability of a binary outcome using a logistic function. Also known as the sigmoid function, this outputs a probability value that is then mapped to two possible classes. \n",
    "\n",
    "It often serves as a good baseline model despite its simplicity, making it a valuable tool for initial analysis before moving on to more complex models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Evaluate (~1m 15s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Logistic Regression model\n",
    "logreg_model = LogisticRegression(random_state=rs)\n",
    "\n",
    "# Train and evaluate the model\n",
    "evaluation_results, logreg_model = train_and_evaluate_model(logreg_model, data_splits, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "---------------------------\n",
      " Dataset   |   Metric   |  Train Test Split  |  Cross Validation  |\n",
      " original  | Accuracy   |      95.874 %      |      95.800 %      |\n",
      "           | Precision  |      99.048 %      |      97.768 %      |\n",
      "           | Recall     |      69.799 %      |      70.294 %      |\n",
      "           | F1         |      81.890 %      |      81.739 %      |\n",
      "\n",
      "   ros     | Accuracy   |      99.016 %      |      98.850 %      |\n",
      "           | Precision  |      98.965 %      |      99.003 %      |\n",
      "           | Recall     |      99.067 %      |      98.694 %      |\n",
      "           | F1         |      99.016 %      |      98.848 %      |\n",
      "\n",
      "   rus     | Accuracy   |      92.977 %      |      93.443 %      |\n",
      "           | Precision  |      98.485 %      |      98.232 %      |\n",
      "           | Recall     |      87.248 %      |      88.494 %      |\n",
      "           | F1         |      92.527 %      |      93.019 %      |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation summary\n",
    "print_evaluation_summary('Logistic Regression', evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results presented:\n",
    "\n",
    "* Both the train-test split and cross-validation methods yielded similar results across all metrics and datasets, suggesting consistency in model performance.\n",
    "\n",
    "**Note**: We will only perform the train-test split method for subsequent models to decrease computation time. \n",
    "\n",
    "1. **Original Dataset**:\n",
    "    * Despite not addressing class imbalance, the original dataset achieved near-perfect accuracy and precision scores. However, it struggled with recall and F1 scores.\n",
    "    * The above indicates potential issues with correctly identifying positive instances.\n",
    "\n",
    "2. **ROS (Random Over-Sampling) Dataset**:\n",
    "    * The ROS dataset consistently outperformed both the original and RUS datasets across all metrics.\n",
    "\n",
    "3. **RUS (Random Under-Sampling) Dataset**:\n",
    "    * The RUS dataset also exhibited improved performance compared to the original, albeit slightly lower than the ROS dataset.\n",
    "\n",
    "Overall Summary:\n",
    "* Both oversampling (ROS) and undersampling (RUS) techniques effectively addressed the class imbalance issue, resulting in improved model performance across all metrics.\n",
    "* The ROS dataset showed the most promising results, achieving high metrics across both train-test split and cross-validation methods.\n",
    "* These findings suggest that addressing class imbalance through sampling techniques significantly enhances the model's ability to generalize and perform well on both training and unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree\n",
    "\n",
    "Decision Tree is a non-linear supervised learning algorithm used for both classification and regression tasks. It works by splitting the data into subsets based on the value of input features, forming a tree-like model of decisions. \n",
    "* Each internal node of the tree represents a decision based on an attribute\n",
    "* Each branch represents the outcome of the decision, and\n",
    "* Each leaf node represents a class label or a continuous value.\n",
    "\n",
    "Unlike linear models, Decision Trees can capture non-linear relationships between features and the target variable. This flexibility allows the model to fit more complex patterns in the data, which can be beneficial for identifying spam emails.\n",
    "\n",
    "Decision Trees perform implicit feature selection by choosing the most informative features for splitting the data. This property helps in reducing the dimensionality and removing irrelevant features from our spam email dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Evaluate (~2m 10s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=rs)\n",
    "\n",
    "# Train and evaluate the Decision Tree model\n",
    "dt_evaluation_results, dt_model = train_and_evaluate_model(dt_model, data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Results\n",
      "---------------------\n",
      " Dataset   |   Metric   |  Train Test Split  |\n",
      " original  | Accuracy   |      95.874 %      |\n",
      "           | Precision  |      85.034 %      |\n",
      "           | Recall     |      83.893 %      |\n",
      "           | F1         |      84.459 %      |\n",
      "\n",
      "   ros     | Accuracy   |      97.824 %      |\n",
      "           | Precision  |      95.829 %      |\n",
      "           | Recall     |     100.000 %      |\n",
      "           | F1         |      97.870 %      |\n",
      "\n",
      "   rus     | Accuracy   |      90.970 %      |\n",
      "           | Precision  |      89.610 %      |\n",
      "           | Recall     |      92.617 %      |\n",
      "           | F1         |      91.089 %      |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation summary\n",
    "print_evaluation_summary('Decision Tree', dt_evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results presented:\n",
    "\n",
    "The ROS dataset once again demonstrated strong performance, with the Decision Tree model achieving high accuracy, precision, recall, and F1 score. While there was a slight decrease in performance compared to the Logistic Regression model, the differences were minimal, indicating that the ROS dataset remains a viable option for building a spam email classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random Forest\n",
    "\n",
    "Random Forest is a versatile ensemble learning method for classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputs the mode of the classes (classification) or the mean prediction (regression) of the individual trees.\n",
    "\n",
    "By aggregating predictions from multiple decision trees, Random Forest reduces the risk of overfitting. It also provides a feature importance score, which indicates the contribution of each feature in making accurate predictions. This can be useful for feature selection and gaining insights into the most influential factors driving the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Evaluate (~2m 5s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train and evaluate the model\n",
    "evaluation_results_rf, rf_model = train_and_evaluate_model(rf_model, data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results\n",
      "---------------------\n",
      " Dataset   |   Metric   |  Train Test Split  |\n",
      " original  | Accuracy   |      97.578 %      |\n",
      "           | Precision  |     100.000 %      |\n",
      "           | Recall     |      81.879 %      |\n",
      "           | F1         |      90.037 %      |\n",
      "\n",
      "   ros     | Accuracy   |      99.948 %      |\n",
      "           | Precision  |      99.896 %      |\n",
      "           | Recall     |     100.000 %      |\n",
      "           | F1         |      99.948 %      |\n",
      "\n",
      "   rus     | Accuracy   |      93.311 %      |\n",
      "           | Precision  |      96.403 %      |\n",
      "           | Recall     |      89.933 %      |\n",
      "           | F1         |      93.056 %      |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Output summary of evaluation metrics\n",
    "print_evaluation_summary('Random Forest', evaluation_results_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results presented:\n",
    "\n",
    "Random Forest excelled on the ROS dataset, achieving near-perfect accuracy, precision, recall, and F1 score, indicating excellent performance on oversampled data. This should be no surprised as mentioned earlier that this model can handle imbalance better.\n",
    "\n",
    "*Note*: Due to the consistency of ROS performance, we will be disregarding the original and RUS datasets to reduce computation time unless future model results indicate a need to test these methods as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gradient Boosting\n",
    "\n",
    "Gradient Boosting is an ensemble learning technique used for regression and classification tasks. It builds a model in a stage-wise fashion, and it generalizes by allowing optimization of an arbitrary differentiable loss function. Each new model attempts to correct the errors made by the previously trained model. This iterative process results in a strong predictive model.\n",
    "\n",
    "For our spam email classification task, Gradient Boosting can potentially capture subtle patterns in the data that simpler models might miss, leading to improved accuracy and robustness in detecting spam emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Evaluate (~8m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove original and rus from data_splits\n",
    "data_splits = {\n",
    "    'ros': data_splits['ros']\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(random_state=rs)\n",
    "\n",
    "# Train and evaluate the model on the datasets\n",
    "evaluation_results, gb_model = train_and_evaluate_model(gb_model, data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Results\n",
      "-------------------------\n",
      " Dataset   |   Metric   |  Train Test Split  |\n",
      "   ros     | Accuracy   |      93.005 %      |\n",
      "           | Precision  |      96.009 %      |\n",
      "           | Recall     |      89.741 %      |\n",
      "           | F1         |      92.769 %      |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation summary\n",
    "print_evaluation_summary(\"Gradient Boosting\", evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results presented:\n",
    "\n",
    "The Gradient Boosting model performed well on the ROS dataset, though it did not achieve the same high levels of performance as the Random Forest model. The precision and recall values suggest a strong ability to correctly identify positive instances, but with slightly more false positives or negatives compared to the Random Forest results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
