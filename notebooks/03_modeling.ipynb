{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "This notebook focuses on the training and evaluation of machine learning models for spam detection. We will use the preprocessed data from the previous step and apply various machine learning algorithms to classify messages as spam or ham. The notebook will cover model selection, training, hyperparameter tuning, and performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#model-training-and-evaluation)\n",
    "2. [Loading the Data](#loading-the-data)\n",
    "3. [Train-Test Split](#train-test-split)\n",
    "4. [Model Selection](#model-selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Preprocessed Data from Preprocessing Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (5572, 2)\n",
      "Original TF-IDF features shape: (5572, 5000)\n",
      "ROS data shape: (9650, 2)\n",
      "ROS TF-IDF features shape: (9650, 5000)\n",
      "RUS data shape: (1494, 2)\n",
      "RUS TF-IDF features shape: (1494, 3847)\n"
     ]
    }
   ],
   "source": [
    "# Define the directory paths\n",
    "processed_data_dir = 'data/processed'\n",
    "original_dir = os.path.join(processed_data_dir, 'original')\n",
    "ros_dir = os.path.join(processed_data_dir, 'ros')\n",
    "rus_dir = os.path.join(processed_data_dir, 'rus')\n",
    "\n",
    "# Load original data\n",
    "df_original = pd.read_csv(os.path.join(original_dir, 'original_data.csv'))\n",
    "X_tfidf_original = np.load(os.path.join(original_dir, 'original_tfidf_features.npy'))\n",
    "y_original = df_original['label']\n",
    "\n",
    "# Load ROS data\n",
    "df_ros = pd.read_csv(os.path.join(ros_dir, 'ros_data.csv'))\n",
    "X_tfidf_ros = np.load(os.path.join(ros_dir, 'ros_tfidf_features.npy'))\n",
    "y_ros = df_ros['label']\n",
    "\n",
    "# Load RUS data\n",
    "df_rus = pd.read_csv(os.path.join(rus_dir, 'rus_data.csv'))\n",
    "X_tfidf_rus = np.load(os.path.join(rus_dir, 'rus_tfidf_features.npy'))\n",
    "y_rus = df_rus['label']\n",
    "\n",
    "datasets = {\n",
    "    'original': (X_tfidf_original, y_original),\n",
    "    'ros': (X_tfidf_ros, y_ros),\n",
    "    'rus': (X_tfidf_rus, y_rus)\n",
    "}\n",
    "\n",
    "# Print the shapes of the loaded data\n",
    "print(\"Original data shape:\", df_original.shape)\n",
    "print(\"Original TF-IDF features shape:\", X_tfidf_original.shape)\n",
    "print(\"ROS data shape:\", df_ros.shape)\n",
    "print(\"ROS TF-IDF features shape:\", X_tfidf_ros.shape)\n",
    "print(\"RUS data shape:\", df_rus.shape)\n",
    "print(\"RUS TF-IDF features shape:\", X_tfidf_rus.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Split the preprocessed data into training and testing sets to evaluate our model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 42\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=rs):\n",
    "    \"\"\"\n",
    "    Split data into training and testing sets.\n",
    "    \n",
    "    ## Parameters\n",
    "    `X`: ndarray \n",
    "        Features.\n",
    "    `y`: ndarray\n",
    "        Labels.\n",
    "    `test_size`: float, default=0.2\n",
    "        Proportion of the dataset to include in the test split.\n",
    "    `random_state`: int, default=42\n",
    "        Random seed for reproducibility.\n",
    "    \n",
    "    ## Returns\n",
    "    `dict`: Dictionary containing training and testing sets (X_train, X_test, y_train, y_test).\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    return {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}\n",
    "\n",
    "# Split the data for all datasets\n",
    "data_splits = {\n",
    "    'original': split_data(X_tfidf_original, y_original),\n",
    "    'ros': split_data(X_tfidf_ros, y_ros),\n",
    "    'rus': split_data(X_tfidf_rus, y_rus)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "In this section, we will explore and evaluate different machine learning models to classify spam emails. We will use various models to understand which one performs best for our dataset. The models we will cover include:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Decision Tree\n",
    "3. Random Forest\n",
    "4. Gradient Boosting\n",
    "5. Support Vector Machine (SVM)\n",
    "6. k-Nearest Neighbors (k-NN)\n",
    "7. Naive Bayes\n",
    "\n",
    "*Note*: Certain algorithms like Random Forests and Gradient Boosting can handle imbalance better.\n",
    "\n",
    "We will analyze the following metrics in our evaluation of each model:\n",
    "* **Accuracy** - The proportion of correctly classified instances among the total number of instances. It provides an overall asssessment of the model's correctness.\n",
    "\n",
    "* **Precision** - Also known as positive predictive value, measures the proportion of correctly predicted positive instances (true positives) among all predicted positive instances (true positives + false positives). It reflects the model's ability to avoid false positives.\n",
    "    - \"Of all the instances predicted as positive, how many are actually positive?\"\n",
    "\n",
    "* **Recall** - Also known as sensitivity or true positive rate, measures the proportion of correctly predicted positive instances (true positives) among all actual positive instances (true positives + false negatives). It reflects the model's ability to identify all relevant instances.\n",
    "    - \"Of all the actual positive instances, how many were correctly predicted as positive?\"\n",
    "\n",
    "* **F1** - The harmonic mean of precision and recall. It provides a balance between precision and recall and is especially useful when dealing with imbalanced datasets.\n",
    "\n",
    "*Note*: Accuracy might not be the best metric. Precision, recall, and F1-score can better evaluate the model's performance on imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a machine learning model.\n",
    "    \n",
    "    ## Parameters\n",
    "    `model`: sklearn estimator object\n",
    "        The model to evaluate.\n",
    "    `X_train`: array-like\n",
    "        Training features.\n",
    "    `X_test`: array-like\n",
    "        Testing features.\n",
    "    `y_train`: array-like\n",
    "        Training labels.\n",
    "    `y_test`: array-like\n",
    "        Testing labels.\n",
    "    \n",
    "    ## Returns\n",
    "    `dict`: Dictionary containing evaluation metrics (accuracy, precision, recall, f1).\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Store evaluation metrics in a dictionary\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(model, X, y, n_splits=10, random_state=rs):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to evaluate the performance of a machine learning model.\n",
    "    \n",
    "    ## Parameters\n",
    "    `model`: sklearn estimator object\n",
    "        The model to evaluate.\n",
    "    `X`: array-like\n",
    "        Features.\n",
    "    `y`: array-like\n",
    "        Labels.\n",
    "    `n_splits`: int, default=10\n",
    "        Number of folds for cross-validation.\n",
    "    `random_state`: int, default=42\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    ## Returns\n",
    "    `dict`: Dictionary containing evaluation metrics (accuracy, precision, recall, f1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize StratifiedKFold cross-validator\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Initialize dictionary to store evaluation metrics\n",
    "    scores = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "    # Iterate over each fold\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Split the data into training and testing sets for the current fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate evaluation metrics for the current fold\n",
    "        scores['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        scores['precision'].append(precision_score(y_test, y_pred))\n",
    "        scores['recall'].append(recall_score(y_test, y_pred))\n",
    "        scores['f1'].append(f1_score(y_test, y_pred))\n",
    "\n",
    "    # Calculate average metrics across all folds\n",
    "    metrics = {}\n",
    "    for metric, values in scores.items():\n",
    "        metrics[metric] = np.mean(values)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression\n",
    "\n",
    "Logistic Regression is a linear model commonly used for binary classification problems. It predicts the probability of a binary outcome using a logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "---------------------------\n",
      " Dataset   |   Metric   |  Train Test Split  |  Cross Validation  |\n",
      " original  | Accuracy   |       0.959        |       0.958        |\n",
      "           | Precision  |       0.990        |       0.978        |\n",
      "           | Recall     |       0.698        |       0.703        |\n",
      "           | F1         |       0.819        |       0.817        |\n",
      "\n",
      "   ros     | Accuracy   |       0.990        |       0.988        |\n",
      "           | Precision  |       0.990        |       0.990        |\n",
      "           | Recall     |       0.991        |       0.987        |\n",
      "           | F1         |       0.990        |       0.988        |\n",
      "\n",
      "   rus     | Accuracy   |       0.930        |       0.934        |\n",
      "           | Precision  |       0.985        |       0.982        |\n",
      "           | Recall     |       0.872        |       0.885        |\n",
      "           | F1         |       0.925        |       0.930        |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the Logistic Regression model\n",
    "logreg_model = LogisticRegression(random_state=rs)\n",
    "\n",
    "# Dictionary to store evaluation metric for each dataset\n",
    "evaluation_results = {}\n",
    "\n",
    "# Iterate over each dataset\n",
    "for (dataset_name, dataset_splits), (X, y) in zip(data_splits.items(), datasets.values()):\n",
    "    # Extract the training and testing splits\n",
    "    X_train, y_train = dataset_splits['X_train'], dataset_splits['y_train']\n",
    "    X_test, y_test = dataset_splits['X_test'], dataset_splits['y_test']\n",
    "\n",
    "    # Evaluate the model using train-test split method\n",
    "    evaluation_results[dataset_name] = {}\n",
    "    evaluation_results[dataset_name]['train_test_split'] = evaluate_model(logreg_model, X_train, X_test, y_train, y_test)\n",
    "    # Evaluate the model using cross-validation method\n",
    "    evaluation_results[dataset_name]['cross_validation'] = cross_validate_model(logreg_model, X, y)\n",
    "\n",
    "# Display evaluation metrics for each dataset and method\n",
    "print(\"Logistic Regression Results\\n\", '-'*27, sep='')\n",
    "print(\"{:^10} | {:^10} | {:^18} | {:^18} |\".format(\"Dataset\", \"Metric\", \"Train Test Split\", \"Cross Validation\"))\n",
    "for dataset_name, methods in evaluation_results.items():\n",
    "    print(f\"{dataset_name:^10} | \", end=\"\")\n",
    "    metrics_split = methods['train_test_split']\n",
    "    metrics_cross = methods['cross_validation']\n",
    "    for metric_name in metrics_split.keys():\n",
    "        v1 = metrics_split[metric_name]\n",
    "        v2 = metrics_cross[metric_name]\n",
    "        if metric_name != 'accuracy':\n",
    "            print(\"{:^10} | \".format(''), end=\"\")\n",
    "        print(\"{:<10} | \".format(metric_name.capitalize()), end=\"\")\n",
    "        print(\"{:^18.3f} | {:^18.3f} |\".format(v1, v2))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results presented:\n",
    "\n",
    "* Both the train-test split and cross-validation methods yielded similar results across all metrics and datasets, suggesting consistency in model performance. \n",
    "\n",
    "1. **Original Dataset**:\n",
    "    * Despite not addressing class imbalance, the original dataset achieved near-perfect accuracy and precision scores. However, it struggled with recall and F1 scores.\n",
    "    * The above indicates potential issues with correctly identifying positive instances.\n",
    "\n",
    "2. **ROS (Random Over-Sampling) Dataset**:\n",
    "    * The ROS dataset consistently outperformed both the original and RUS datasets across all metrics.\n",
    "\n",
    "3. **RUS (Random Under-Sampling) Dataset**:\n",
    "    * The RUS dataset also exhibited improved performance compared to the original, albeit slightly lower than the ROS dataset.\n",
    "\n",
    "Overall Summary:\n",
    "* Both oversampling (ROS) and undersampling (RUS) techniques effectively addressed the class imbalance issue, resulting in improved model performance across all metrics.\n",
    "* The ROS dataset showed the most promising results, achieving high metrics across both train-test split and cross-validation methods.\n",
    "* These findings suggest that addressing class imbalance through sampling techniques significantly enhances the model's ability to generalize and perform well on both training and unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
